{{- if .Values.zfs.enabled -}}
apiVersion: v1
kind: ConfigMap
metadata:
  labels: {{ include "hedera-mirror-common.labels" . | nindent 4 }}
    app: zfs-init
  name: {{ .Release.Name }}-zfs-init
  namespace: {{ include "hedera-mirror-common.namespace" . }}
data:
  label-wait.sh: |
    #!/usr/bin/env bash
    set -euxo pipefail
    
    ROOT_MOUNT_DIR="${ROOT_MOUNT_DIR:-/node}"
    ZFS_INITIALIZED="${ZFS_INITIALIZED:-}"
    until [ "${ZFS_INITIALIZED}" != "" ]; do echo "Waiting for label "; sleep 10; source "${ROOT_MOUNT_DIR}/etc/environment"; done

  entrypoint.sh: |
    #!/usr/bin/env bash
    
    set -uxo pipefail
    
    ROOT_MOUNT_DIR="${ROOT_MOUNT_DIR:-/node}"
    DISK_SIZE_COORDINATOR="${DISK_SIZE_COORDINATOR:-75GB}"
    DISK_SIZE_WORKER="${DISK_SIZE_WORKER:-152GB}"
    K8S_NAMESPACE="${K8S_NAMESPACE:-common}"
    DISK_PREFIX="${DISK_PREFIX:-citus}"
    CONFIG_MAP_NAME="{{ include "hedera-mirror-common.fullname" . }}-zfs-node-status"
    
    NODE_ID_LABEL="openebs.io/nodeid"
    ROLE_LABEL="citus-role"

    echo "Installing dependencies"
    apt-get update
    apt-get install -y kubectl jq
    echo "Getting node metadata"
    NODE_NAME="$(curl -sS http://metadata.google.internal/computeMetadata/v1/instance/hostname -H 'Metadata-Flavor: Google' | awk -F'.' '{print $1}')"
    ZONE="$(curl -sS http://metadata.google.internal/computeMetadata/v1/instance/zone -H 'Metadata-Flavor: Google' | awk -F  "/" '{print $NF}')"
    NODE_LABELS="$(kubectl get node ${NODE_NAME} -o jsonpath='{.metadata.labels}')"
    CITUS_ROLE="$(echo "${NODE_LABELS}" | jq --arg ROLE_LABEL "${ROLE_LABEL}" -r '.[$ROLE_LABEL]')"
    NODE_ID="$(echo "${NODE_LABELS}" | jq --arg NODE_ID_LABEL "${NODE_ID_LABEL}" -r '.[$NODE_ID_LABEL]')"

    ATTEMPT_COUNT=1
    while [[ "${CITUS_ROLE}" == "null" ]]
    do
      if [[ ${ATTEMPT_COUNT} -ge 13 ]]
      then
        echo "Timed out waiting for labels please set ${ROLE_LABEL}"
        exit 1
      fi
      sleep 10
      ATTEMPT_COUNT=$((ATTEMPT_COUNT+1))
      echo "Retrying label GET attempt: ${ATTEMPT_COUNT}"
      NODE_LABELS="$(kubectl get node ${NODE_NAME} -o jsonpath='{.metadata.labels}')"
      CITUS_ROLE="$(echo "${NODE_LABELS}" | jq --arg ROLE_LABEL "${ROLE_LABEL}" -r '.[$ROLE_LABEL]')"
      NODE_ID="$(echo "${NODE_LABELS}" | jq --arg NODE_ID_LABEL "${NODE_ID_LABEL}" -r '.[$NODE_ID_LABEL]')"
    done
    # Determine correct config for zone
    COMMAND_STATUS=1
    until [[ ${COMMAND_STATUS} -eq 0 ]]; do
      CONFIG_MAP="$(kubectl get configmap -o json -n ${K8S_NAMESPACE} ${CONFIG_MAP_NAME})"
      MODIFIED_CONFIG_MAP="$(echo "${CONFIG_MAP}" |jq '{metadata: {resourceVersion: (.metadata.resourceVersion)}, data: {"zfs-node-status.json": (.data["zfs-node-status.json"]|fromjson)}}')"
      ZONE_CONFIG="$(echo "${MODIFIED_CONFIG_MAP}" |jq  --arg ZONE "${ZONE}" --arg CITUS_ROLE "${CITUS_ROLE}" '.data["zfs-node-status.json"][$CITUS_ROLE][$ZONE] // []')"
      NODE_LIST="$(kubectl get nodes -o jsonpath='{.items[*].metadata.labels}' -lcsi-type=zfs,topology.kubernetes.io/zone=${ZONE},${ROLE_LABEL}=${CITUS_ROLE} | jq -s)"
      NODE_NAMES="$(echo "${NODE_LIST}" | jq '. | map({(.["kubernetes.io/hostname"]): {isUpgrade: (.["operation.gke.io/type"] == "drain")}}) | add')"
      
      ## This will find all nodes that have been initialized by this script in the past
      ## and generate its own version of the config map configuration to 
      ZFS_ZONE_NODES=$(echo "${NODE_LIST}" | jq \
                      --arg NODE_ID_LABEL "${NODE_ID_LABEL}" \
                      '. | map(select(.["generated-node-id"] == "true")) |
                           map({
                                 "nodeName": (.["kubernetes.io/hostname"]), 
                                 "nodeId": (.[$NODE_ID_LABEL]), 
                                 "index": (.[$NODE_ID_LABEL] | match("\\d*$").string | tonumber)
                               }) | 
                            sort_by(.index) |
                              [
                                range(0; ( .| max_by(.index)|.index ) + 1) as $node_index_range |
                                  . as $nodes | $node_index_range |
                                  . as $i |
                                  if ([$nodes[]|contains( { index: $i })]|any )
                                     then 
                                     ($nodes|map(select( .index  == $i )))|map(. | {nodeName, nodeId})|.[0] 
                                  else 
                                     null
                                  end  
                              ]')
      # Check if the config map matches the view from the nodes. If it finds entries
      # in the config map for nodes that don't exist or are currrently being drained, they are set to null to become 
      # available slots for future nodes that may replace it
      MERGED_ZONE_CONFIG=$(echo -e "${NODE_NAMES}\n${ZFS_ZONE_NODES}\n${ZONE_CONFIG}" | jq -s \
                      '.[0] as $node_names|
                        .[1] as $node_config|
                        .[2] as $config_map|
                        [
                          range(0; [(.[2] |length), (.[1] |length)]|max) as $index_range|
                          $index_range |
                            . as $i|
                            if ($config_map[$i] != $node_config[$i])
                              then
                                  if($config_map[$i] != null and $node_names[$config_map[$i].nodeName] != null)
                                    then 
                                      $config_map[$i] 
                                  else 
                                      $node_config[$i] 
                                  end
                            else
                              $node_config[$i]
                            end | 
                            if (. != null and $node_names[.nodeName].isUpgrade) then null else . end
                        ]')
      THIS_NODE_CONFIG="$(echo "${MERGED_ZONE_CONFIG}" |jq  --arg NODE_NAME "${NODE_NAME}" --arg ZONE "${ZONE}" '.[]?|select(.nodeName == $NODE_NAME)')"
      # IF this node config is empty create it
      if [[ ! $THIS_NODE_CONFIG ]]
      then
        INDEX="$(echo "${MERGED_ZONE_CONFIG}" | jq '. | map(. == null) |index(true) // . | length')"
        NODE_ID="${CITUS_ROLE}-${ZONE}-${INDEX}"
        THIS_NODE_CONFIG="{\"nodeName\": \"$NODE_NAME\", \"nodeId\": \"$NODE_ID\"}"
        MERGED_ZONE_CONFIG="$(echo "${MERGED_ZONE_CONFIG}" | jq --arg INDEX $INDEX --arg THIS_NODE_CONFIG "${THIS_NODE_CONFIG}"  '.[$INDEX |tonumber] = ($THIS_NODE_CONFIG|fromjson)')"
      fi
        
      MODIFIED_CONFIG_MAP="$(echo "${MODIFIED_CONFIG_MAP}" | jq --arg CITUS_ROLE "${CITUS_ROLE}"  --arg ZONE "${ZONE}" --arg ZONE_CONFIG "${MERGED_ZONE_CONFIG}" '.data["zfs-node-status.json"][$CITUS_ROLE][$ZONE]=($ZONE_CONFIG|fromjson)')"
      MODIFIED_CONFIG_MAP="$(echo "${MODIFIED_CONFIG_MAP}" | jq '.data["zfs-node-status.json"]=(.data["zfs-node-status.json"]|tojson)')"
    
      echo "patching $CONFIG_MAP_NAME with $MODIFIED_CONFIG_MAP"
      kubectl patch configmap $CONFIG_MAP_NAME \
      -n $K8S_NAMESPACE \
      --type merge \
      -p "${MODIFIED_CONFIG_MAP}"
      COMMAND_STATUS=$?
    done
    
    DISK_NAME="${DISK_PREFIX}-${NODE_ID}-zfs"
    echo "Setting up disk ${DISK_NAME} for ${CITUS_ROLE} on zfs node ${NODE_ID}"
    if [[ "$CITUS_ROLE" == *"worker"* ]]; then
        DISK_SIZE="${DISK_SIZE_WORKER}"
    else
        DISK_SIZE="${DISK_SIZE_COORDINATOR}"
    fi
    
    ACTUAL_SIZE=$(gcloud compute disks list --filter="name:${DISK_NAME}" --format="value(sizeGb)")
    if [[ -z "${ACTUAL_SIZE}" ]]; then
        echo "Creating ${DISK_NAME} for ${CITUS_ROLE} with size ${DISK_SIZE}"
        gcloud compute disks create "${DISK_NAME}" --size="${DISK_SIZE}" --zone="${ZONE}" --type=pd-balanced --quiet
    else
        echo "${DISK_NAME} already exists for ${CITUS_ROLE}"
    fi

    if [[ "${ACTUAL_SIZE}" -lt "${DISK_SIZE%GB}" ]]; then
        echo "Increasing disk size from ${ACTUAL_SIZE} to ${DISK_SIZE}"
        gcloud compute disks resize "${DISK_NAME}" --size="${DISK_SIZE}" --zone="${ZONE}" --quiet
    fi

    if (! gcloud compute instances describe "${NODE_NAME}" --zone "${ZONE}" --format '(disks[].source)' | grep "${DISK_NAME}" > /dev/null); then
        echo "Attaching ${DISK_NAME} to ${NODE_NAME}"
        gcloud compute instances attach-disk "${NODE_NAME}" --device-name=sdb --disk "${DISK_NAME}" --zone "${ZONE}"
        COMMAND_STATUS=$?
        ATTACH_ATTEMPTS=1
        until [[ ${COMMAND_STATUS} -eq 0 || ${ATTACH_ATTEMPTS} -ge 13 ]]; do
          echo "Unable to attach ${DISK_NAME} to ${NODE_NAME} attempts ${ATTACH_ATTEMPTS}. waiting ..." 
          sleep 30
          gcloud compute instances attach-disk "${NODE_NAME}" --device-name=sdb --disk "${DISK_NAME}" --zone "${ZONE}"
          COMMAND_STATUS=$?
          ATTACH_ATTEMPTS=$((ATTACH_ATTEMPTS+1))
        done
    
        if [[ $COMMAND_STATUS != 0 ]]; then
          echo "Unable to attach ${DISK_NAME} to ${NODE_NAME} in ${ZONE}";
          exit 1
        fi
    else
        echo "${DISK_NAME} is already attached to ${NODE_NAME}"
    fi

    chroot "${ROOT_MOUNT_DIR}" /bin/bash -x <<'EOF'

      echo "Installing zfs"
      apt-get update
      apt-get install -y zfsutils-linux
      export POOL="{{ .Values.zfs.parameters.poolname }}"
      echo "Configuring zpool ${POOL}"
      partprobe

      if (zfs list | grep -q "${POOL}"); then
          echo "Found existing pool. Skipping creation"
      elif (zpool create -o autoexpand=on "${POOL}" /dev/sdb); then
          echo "Successfully created pool"
      elif (zpool import -f "${POOL}"); then
          echo "Successfully imported pool"
      else
          echo "Unable to create pool. Manual intervention necessary"
          exit 1
      fi

      zpool set autoexpand=on "${POOL}"
      zpool online -e "${POOL}" /dev/sdb
      zfs list
    EOF
    
    CURRENT_NODE_ID="$(echo "${NODE_LABELS}" | jq --arg NODE_ID_LABEL "${NODE_ID_LABEL}" -r '.[$NODE_ID_LABEL]')"
    if [[ "${CURRENT_NODE_ID}" != "${NODE_ID}" ]]
    then
      echo "Labeling node ${NODE_NAME} with ${NODE_ID_LABEL}=${NODE_ID}"
      kubectl label node "${NODE_NAME}" "${NODE_ID_LABEL}=${NODE_ID}" generated-node-id=true --overwrite
    fi
    
    source "${ROOT_MOUNT_DIR}/etc/environment"
    ZFS_INITIALIZED="${ZFS_INITIALIZED:-}"
    if [[ "${ZFS_INITIALIZED}" == "" ]]; then
      echo "ZFS_INITIALIZED=\"true\"" > "${ROOT_MOUNT_DIR}/etc/environment"  
    fi

{{- end -}}