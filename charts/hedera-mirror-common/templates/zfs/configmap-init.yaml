{{- if .Values.zfs.enabled -}}
apiVersion: v1
kind: ConfigMap
metadata:
  labels: {{ include "hedera-mirror-common.selectorLabels" . | nindent 4 }}
    app: zfs-init
  name: {{ .Release.Name }}-zfs-init
  namespace: {{ include "hedera-mirror-common.namespace" . }}
data:
  label-wait.sh: |
    #!/usr/bin/env bash
    set -euxo pipefail

    ROOT_MOUNT_DIR="${ROOT_MOUNT_DIR:-/node}"
    ZFS_INITIALIZED="${ZFS_INITIALIZED:-}"
    until [ "${ZFS_INITIALIZED}" != "" ]; do echo "Waiting for label "; sleep 10; source "${ROOT_MOUNT_DIR}/etc/environment"; done

  entrypoint.sh: |
    #!/usr/bin/env bash

    set -euxo pipefail

    # Constants
    NODE_ID_LABEL="openebs.io/nodeid"
    ROLE_LABEL="citus-role"
    ENV_FILE="${ROOT_MOUNT_DIR}/etc/environment"

    # Configurable environment variables
    CONFIG_MAP_NAME="{{ include "hedera-mirror-common.fullname" . }}-zfs-node-status"
    CONFIG_MAP_KEY="zfs-node-status.json"
    DISK_PREFIX="${DISK_PREFIX:-citus}"
    DEFAULT_DISK_SIZE_COORDINATOR="${DEFAULT_DISK_SIZE_COORDINATOR:-75GB}"
    DEFAULT_DISK_SIZE_WORKER="${DEFAULT_DISK_SIZE_WORKER:-152GB}"
    K8S_NAMESPACE="${K8S_NAMESPACE:-common}"
    ROOT_MOUNT_DIR="${ROOT_MOUNT_DIR:-/node}"

    echo "Installing dependencies"
    apt-get update
    apt-get install -y kubectl jq
    echo "Getting node metadata"
    NODE_NAME="$(curl -sS http://metadata.google.internal/computeMetadata/v1/instance/hostname -H 'Metadata-Flavor: Google' | awk -F'.' '{print $1}')"
    ZONE="$(curl -sS http://metadata.google.internal/computeMetadata/v1/instance/zone -H 'Metadata-Flavor: Google' | awk -F  "/" '{print $NF}')"
    NODE_LABELS="$(kubectl get node ${NODE_NAME} -o jsonpath='{.metadata.labels}')"
    CITUS_ROLE="$(echo "${NODE_LABELS}" | jq --arg ROLE_LABEL "${ROLE_LABEL}" -r '.[$ROLE_LABEL]')"

    if [[ "$CITUS_ROLE" == *"worker"* ]]; then
      DISK_SIZE="${DEFAULT_DISK_SIZE_WORKER}"
    else
      DISK_SIZE="${DEFAULT_DISK_SIZE_COORDINATOR}"
    fi

    ATTEMPT_COUNT=1
    while [[ "${CITUS_ROLE}" == "null" ]]
    do
      if [[ ${ATTEMPT_COUNT} -ge 13 ]]
      then
        echo "Timed out waiting for labels please set ${ROLE_LABEL}"
        exit 1
      fi
      sleep 10
      ATTEMPT_COUNT=$((ATTEMPT_COUNT+1))
      echo "Retrying label GET attempt: ${ATTEMPT_COUNT}"
      NODE_LABELS="$(kubectl get node ${NODE_NAME} -o jsonpath='{.metadata.labels}')"
      CITUS_ROLE="$(echo "${NODE_LABELS}" | jq --arg ROLE_LABEL "${ROLE_LABEL}" -r '.[$ROLE_LABEL]')"
    done
    # Ensure node status config map is created
    until (kubectl get configmap -o json -n "${K8S_NAMESPACE}" "${CONFIG_MAP_NAME}" &> /dev/null)
    do
      echo "Creating config map ${CONFIG_MAP_NAME}"
      kubectl create configmap "${CONFIG_MAP_NAME}" -n "${K8S_NAMESPACE}" --from-literal="${CONFIG_MAP_KEY}={}" || true
      sleep 1
    done
    COMMAND_STATUS=1
    until [[ ${COMMAND_STATUS} -eq 0 ]]; do
      CONFIG_MAP="$(kubectl get configmap -o json -n ${K8S_NAMESPACE} ${CONFIG_MAP_NAME})"
      MODIFIED_CONFIG_MAP="$(echo "${CONFIG_MAP}" |jq --arg KEY "${CONFIG_MAP_KEY}" '{metadata: {resourceVersion: (.metadata.resourceVersion)}, data: {($KEY): (.data[$KEY]|fromjson)}}')"
      ZONE_CONFIG="$(echo "${MODIFIED_CONFIG_MAP}" |jq --arg KEY "${CONFIG_MAP_KEY}"  --arg ZONE "${ZONE}" --arg CITUS_ROLE "${CITUS_ROLE}" '.data[$KEY][$CITUS_ROLE][$ZONE] // []')"
      NODE_LIST="$(kubectl get nodes -o jsonpath='{.items[*].metadata.labels}' -lcsi-type=zfs,topology.kubernetes.io/zone=${ZONE},${ROLE_LABEL}=${CITUS_ROLE} | jq -s)"
      NODE_NAMES="$(echo "${NODE_LIST}" | jq '. | map({(.["kubernetes.io/hostname"]): {isUpgrade: (.["operation.gke.io/type"] == "drain")}}) | add')"

      ## This will find all nodes that have been initialized by this script in the past
      ## and generate its own version of the config map configuration to 
      ZFS_ZONE_NODES=$(echo "${NODE_LIST}" | jq \
                      --arg NODE_ID_LABEL "${NODE_ID_LABEL}" \
                      '. | map(select(.["generated-node-id"] == "true")) |
                           map({
                                 "nodeName": (.["kubernetes.io/hostname"]), 
                                 "nodeId": (.[$NODE_ID_LABEL]),
                                 "index": (.[$NODE_ID_LABEL] | match("\\d*$").string | tonumber)
                               }) | 
                            sort_by(.index) |
                              [
                                range(0; ( .| max_by(.index)|.index ) + 1) as $node_index_range |
                                  . as $nodes | $node_index_range |
                                  . as $i |
                                  if ([$nodes[]|contains( { index: $i })]|any )
                                     then 
                                     ($nodes|map(select( .index  == $i )))|map(. | {nodeName, nodeId, diskSize})|.[0] 
                                  else 
                                     null
                                  end  
                              ]')
      # Check if the config map matches the view from the nodes. If it finds entries
      # in the config map for nodes that don't exist or are currently being drained, they are set to null to become 
      # available slots for future nodes that may replace it
      MERGED_ZONE_CONFIG=$(echo -e "${NODE_NAMES}\n${ZFS_ZONE_NODES}\n${ZONE_CONFIG}" | jq -s \
                      '.[0] as $node_names|
                        .[1] as $node_config|
                        .[2] as $config_map|
                        [
                          range(0; [(.[2] |length), (.[1] |length)]|max) as $index_range|
                          $index_range |
                            . as $i|
                            if ($config_map[$i] != $node_config[$i])
                              then
                                  if($config_map[$i] != null and $node_names[$config_map[$i].nodeName] != null)
                                    then 
                                      $config_map[$i] 
                                  else 
                                      $node_config[$i] 
                                  end
                            else
                              $node_config[$i]
                            end | 
                            if (. != null and $node_names[.nodeName].isUpgrade) then null else . end
                        ]')
      THIS_NODE_CONFIG="$(echo "${MERGED_ZONE_CONFIG}" |jq  --arg NODE_NAME "${NODE_NAME}" --arg ZONE "${ZONE}" '.[]?|select(.nodeName == $NODE_NAME)')"
      # If there is no entry in the config map, and the node wasn't previously labeled, it is a new node. Create the config.
      if [[ ! $THIS_NODE_CONFIG ]]
      then
        INDEX="$(echo "${MERGED_ZONE_CONFIG}" | jq '. | map(. == null) |index(true) // . | length')"
        NODE_ID="${CITUS_ROLE}-${ZONE}-${INDEX}"
        THIS_NODE_CONFIG="{\"nodeName\": \"$NODE_NAME\", \"nodeId\": \"$NODE_ID\"}"
        MERGED_ZONE_CONFIG="$(echo "${MERGED_ZONE_CONFIG}" | jq --arg INDEX $INDEX --arg THIS_NODE_CONFIG "${THIS_NODE_CONFIG}"  '.[$INDEX |tonumber] = ($THIS_NODE_CONFIG|fromjson)')"
      fi

      MODIFIED_CONFIG_MAP="$(echo "${MODIFIED_CONFIG_MAP}" | jq --arg KEY "${CONFIG_MAP_KEY}" --arg CITUS_ROLE "${CITUS_ROLE}"  --arg ZONE "${ZONE}" --arg ZONE_CONFIG "${MERGED_ZONE_CONFIG}" '.data[$KEY][$CITUS_ROLE][$ZONE]=($ZONE_CONFIG|fromjson)')"
      MODIFIED_CONFIG_MAP="$(echo "${MODIFIED_CONFIG_MAP}" | jq --arg KEY "${CONFIG_MAP_KEY}" '.data[$KEY]=(.data[$KEY]|tojson)')"

      echo "patching $CONFIG_MAP_NAME with $MODIFIED_CONFIG_MAP"
      if (kubectl patch configmap $CONFIG_MAP_NAME -n $K8S_NAMESPACE --type merge -p "${MODIFIED_CONFIG_MAP}")
      then
        echo "Successfully patched config map"
        COMMAND_STATUS=0
      else
        COMMAND_STATUS=$?
        echo "Conflict patching config map. Retrying ..."
      fi
    done

    NODE_ID="$(echo "${THIS_NODE_CONFIG}" | jq -r '.nodeId')"
    if [[ "${NODE_ID}" == "null" ]]
    then
      echo "Unable to determine correct node id. Exiting ..."
      exit 1
    fi

    DISK_NAME="${DISK_PREFIX}-${NODE_ID}-zfs"
    ACTUAL_SIZE=$(gcloud compute disks list --filter="name:${DISK_NAME}" --format="value(sizeGb)")
    if [[ -z "${ACTUAL_SIZE}" ]]; then
        echo "Creating ${DISK_NAME} for ${CITUS_ROLE} with size ${DISK_SIZE}"
        gcloud compute disks create "${DISK_NAME}" --size="${DISK_SIZE}" --zone="${ZONE}" --type=pd-balanced --quiet
    else
        echo "${DISK_NAME} already exists for ${CITUS_ROLE}"
    fi

    if [[ -n $ACTUAL_SIZE && "${ACTUAL_SIZE}" -lt "${DISK_SIZE%GB}" ]]; then
        echo "Increasing disk size from ${ACTUAL_SIZE} to ${DISK_SIZE}"
        gcloud compute disks resize "${DISK_NAME}" --size="${DISK_SIZE}" --zone="${ZONE}" --quiet
    fi

    if (! gcloud compute instances describe "${NODE_NAME}" --zone "${ZONE}" --format '(disks[].source)' | grep "${DISK_NAME}" > /dev/null); then
        echo "Attaching ${DISK_NAME} to ${NODE_NAME}"
        ATTACH_ATTEMPTS=0
        until (gcloud compute instances attach-disk "${NODE_NAME}" --device-name=sdb --disk "${DISK_NAME}" --zone "${ZONE}"); do
          ATTACH_ATTEMPTS=$((ATTACH_ATTEMPTS+1))
          if [[ $ATTACH_ATTEMPTS -ge 15 ]]; then
              echo "Unable to attach ${DISK_NAME} to ${NODE_NAME} in ${ZONE} ater ${ATTACH_ATTEMPTS} attempts. Exiting ..."
              exit 1
          fi
          echo "Unable to attach ${DISK_NAME} to ${NODE_NAME} attempts ${ATTACH_ATTEMPTS}. waiting ..." 
          sleep 30
        done
    else
        echo "${DISK_NAME} is already attached to ${NODE_NAME}"
    fi

    chroot "${ROOT_MOUNT_DIR}" /bin/bash -x <<'EOF'

      echo "Installing zfs"
      apt-get update
      apt-get install -y zfsutils-linux
      export POOL="{{ .Values.zfs.parameters.poolname }}"
      echo "Configuring zpool ${POOL}"
      partprobe

      if (zfs list | grep -q "${POOL}"); then
          echo "Found existing pool. Skipping creation"
      elif (zpool create -o autoexpand=on "${POOL}" /dev/sdb); then
          echo "Successfully created pool"
      elif (zpool import -f "${POOL}"); then
          echo "Successfully imported pool"
      else
          echo "Unable to create pool. Manual intervention necessary"
          exit 1
      fi

      ATTACHED_CACHE_DEVICE="$(zpool status "${POOL}" |grep -A1 -E '^\s*cache\s*$' | grep -v cache | awk '{print $1;}')"
      if [[ -n "${L2_ARC_NVME_DEVICE_ID}" ]]; then
        NVME_DEVICE_PATH="$(readlink -f /dev/disk/by-id/google-local-ssd-block${L2_ARC_NVME_DEVICE_ID})"
        if [[ -z "${ATTACHED_CACHE_DEVICE}" ]]; then
          echo "Setting up NVME device $NVME_DEVICE_PATH as L2 ARC"
          zpool add "${POOL}" cache "$NVME_DEVICE_PATH"
          ATTACHED_CACHE_DEVICE="$(zpool status "${POOL}" |grep -A1 -E '^\s*cache\s*$' | grep -v cache | awk '{print $1;}')"
          if [[ -z "${ATTACHED_CACHE_DEVICE}" ]]; then
            echo "Unable to attach NVME device ${NVME_DEVICE_PATH} as L2 ARC"
            exit 1
          else
            echo "Successfully attached L2 ARC device ${NVME_DEVICE_PATH} as L2 ARC"
          fi
        else
            echo "NVME device ${NVME_DEVICE_PATH} already attached as L2 ARC"
        fi
      elif [[ -n "${ATTACHED_CACHE_DEVICE}" ]]; then
        echo "Previously configured L2 ARC device ${ATTACHED_CACHE_DEVICE}. Is being removed as cache"
        zpool remove "${POOL}" "${ATTACHED_CACHE_DEVICE}"
        ATTACHED_CACHE_DEVICE="$(zpool status "${POOL}" |grep -A1 -E '^\s*cache\s*$' | grep -v cache | awk '{print $1;}')"
        if [[ -n "${ATTACHED_CACHE_DEVICE}" ]]; then
          echo "Unable to remove L2 ARC device ${ATTACHED_CACHE_DEVICE}"
          exit 1
        else
          echo "Successfully removed L2 ARC device ${ATTACHED_CACHE_DEVICE}"
        fi
      else
        echo "No L2 cache device specified. Skipping ..."
      fi

      ARC_SIZE_GB="${ARC_SIZE_GB:-2}"
      echo "Configuring arc to ${ARC_SIZE_GB}GB"
      ARC_SIZE="$((ARC_SIZE_GB*1073741824))"
      echo $ARC_SIZE >> /sys/module/zfs/parameters/zfs_arc_max

      zfs set primarycache=metadata xattr=sa "${POOL}"
      zpool set autoexpand=on "${POOL}"
      zpool online -e "${POOL}" /dev/sdb
      zfs list
    EOF

    CURRENT_NODE_ID="$(echo "${NODE_LABELS}" | jq --arg NODE_ID_LABEL "${NODE_ID_LABEL}" -r '.[$NODE_ID_LABEL]')"
    if [[ "${CURRENT_NODE_ID}" != "${NODE_ID}" ]]
    then
      echo "Labeling node ${NODE_NAME} with ${NODE_ID_LABEL}=${NODE_ID}"
      kubectl label node "${NODE_NAME}" "${NODE_ID_LABEL}=${NODE_ID}" generated-node-id=true --overwrite
    fi

    source "${ENV_FILE}"
    ZFS_INITIALIZED="${ZFS_INITIALIZED:-}"
    if [[ "${ZFS_INITIALIZED}" == "" ]]; then
      echo "ZFS_INITIALIZED=\"true\"" > "${ENV_FILE}"  
    fi

    {{- end -}}