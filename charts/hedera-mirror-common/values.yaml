global:
  namespaceOverride: ""

labels: {}

loki:
  backend:
    replicas: 0
  chunksCache:
    enabled: false
  compactor:
    replicas: 0
  deploymentMode: SingleBinary
  distributor:
    replicas: 0
  enabled: true
  gateway:
    enabled: false
  indexGateway:
    replicas: 0
  ingester:
    replicas: 0
  loki:
    auth_enabled: false
    commonConfig:
      replication_factor: 1
    storage:
      type: 'filesystem'
    structuredConfig:
      compactor:
        delete_request_store: filesystem
        retention_enabled: true
      limits_config:
        retention_period: 1440h
      ruler:
        alertmanager_url: http://{{ .Release.Name }}-prometheus-alertmanager:9093
        enable_alertmanager_v2: true
        enable_api: true
        ring:
          kvstore:
            store: inmemory
        rule_path: /tmp/scratch
        storage:
          type: local
          local:
            directory: /rules
      schema_config:
        configs:
          - from: "2022-01-11"
            index:
              prefix: loki_index_
              period: 24h
            object_store: filesystem
            schema: v13
            store: tsdb
  lokiCanary:
    enabled: false
  monitoring:
    selfMonitoring:
      enabled: false
      grafanaAgent:
        installOperator: false
  querier:
    replicas: 0
  queryFrontend:
    replicas: 0
  queryScheduler:
    replicas: 0
  read:
    replicas: 0
  resultsCache:
    enabled: false
  ruler:
    replicas: 0
  rules:
    - name: hedera-mirror-grpc
      rules:
        - alert: GrpcLogErrors
          annotations:
            description: "Logs for {{ $labels.cluster }}/{{ $labels.namespace }}/{{ $labels.pod }} have reached {{ $value }} error messages/s in a 3m period"
            summary: High rate of errors in logs
          expr: >
            sum(rate({component="grpc"}
              | regexp `(?P<timestamp>\S+)\s+(?P<level>\S+)\s+(?P<thread>\S+)\s+(?P<class>\S+)\s+(?P<message>.+)`
              | level = "ERROR"
              != "reactor.core.publisher"
              != "drainError"
              != "org.springframework"
              != "connection validation failed"
              != "Stream closed before write could take place"
              != "Connection unexpectedly closed"
              != "Unknown error subscribing to topic"
              != "Error has been observed at the following"
              != "ReactorNettyClient$PostgresConnectionClosedException: Connection closed"
              != "i.g.n.s.i.g.n.NettyServerHandler Stream Error"
              != "Http2Exception.streamError"
              != "readAddress(..) failed: Connection reset by peer"
              != "i.r.p.c.ReactorNettyClient Connection Error"
              != "i.n.u.ResourceLeakDetector LEAK: ByteBuf.release()"
              != "Not a valid topic"
              != "Must be greater than or equal to 0"
              != "Topic does not exist"
            [1m])) by (cluster, namespace, pod) > 1
          for: 3m
          labels:
            severity: critical
    - name: hedera-mirror-importer
      rules:
        - alert: ImporterLogErrors
          annotations:
            description: "Logs for {{ $labels.cluster }}/{{ $labels.namespace }}/{{ $labels.pod }} have reached {{ $value }} error messages/s in a 5m period"
            summary: High rate of errors in logs
          expr: >
            sum(rate({component="importer"}
              | regexp `(?P<timestamp>\S+)\s+(?P<level>\S+)\s+(?P<thread>\S+)\s+(?P<class>\S+)\s+(?P<message>.+)`
              | level = "ERROR"
              != "UnusedChannelExceptionHandler"
              | message =~ ".*(Exception|hash mismatch for file|Unknown record file delimiter|Unknown file delimiter|Error parsing record file|Expecting previous file hash|Unable to extract hash and signature from file|Failed to verify record files|Account balance dataset timestamp mismatch!|ERRORS processing account balances file|does not exist in the database|Unable to connect to database|Address book file).*"
            [1m])) by (cluster, namespace, pod) > 0.5
          for: 5m
          labels:
            severity: critical
        - alert: ImporterRecoverableErrors
          annotations:
            description: "Recoverable Error Logs for {{ $labels.cluster }}/{{ $labels.namespace }}/{{ $labels.pod }} have reached {{ $value }} error messages/s in a 1m period"
            summary: Recoverable Error found in logs
          expr: >
            sum(count_over_time({component="importer"}
              | regexp `(?P<timestamp>\S+)\s+(?P<level>\S+)\s+(?P<thread>\S+)\s+(?P<class>\S+)\s+(?P<message>.+)`
              | level = "ERROR"
              | message =~ ".*Recoverable error.*"
            [1m])) by (cluster, namespace, pod) > 0
          labels:
            severity: critical
    - name: hedera-mirror-rest
      rules:
        - alert: RestLogErrors
          annotations:
            description: "Logs for {{ $labels.cluster }}/{{ $labels.namespace }}/{{ $labels.pod }} have reached {{ $value }} error messages/s in a 1m period"
            summary: "High rate of log errors"
          expr: >
            sum(rate({component="rest"}
              | regexp `(?P<timestamp>\S+)\s+(?P<level>\S+)\s+(?P<requestId>\S+)\s+(?P<message>.+)`
              | level = "ERROR" or level = "FATAL"
              != "canceling statement due to statement timeout"
            [1m])) by (cluster, namespace, pod) > 0.04
          for: 1m
          labels:
            severity: critical
    - name: hedera-mirror-rosetta
      rules:
        - alert: RosettaLogErrors
          annotations:
            description: "Logs for {{ $labels.cluster }}/{{ $labels.namespace }}/{{ $labels.pod }} have reached {{ $value }} error messages/s in a 1m period"
            summary: "High rate of log errors"
          expr: >
            sum(rate({component="rosetta"}
              | logfmt
              | level = "error" or level = "fatal"
            [1m])) by (cluster, namespace, pod) > 0.04
          for: 1m
          labels:
            severity: critical
  serviceMonitor:
    enabled: true
  singleBinary:
    extraVolumeMounts:
      - mountPath: /rules
        name: rules
    extraVolumes:
      - name: rules
        configMap:
          defaultMode: 420
          name: mirror-log-alerts
    persistence:
      enableStatefulSetAutoDeletePVC: true
      size: 250Gi
    replicas: 1
    resources:
      limits:
        cpu: 200m
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 128Mi
  test:
    enabled: false
  write:
    replicas: 0

minio:
  enabled: false
  persistence:
    enabled: true
    size: 500Gi
    storageClass: premium-rwo
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      interval: 60s
      jobLabel: minio
  priorityClassName: critical
  provisioning:
    enabled: true
    buckets:
      - name: stackgres-backup
  resources:
    requests:
      cpu: 1000m
      memory: 2Gi

networkPolicy:
  enabled: false

prometheus-adapter:
  enabled: true
  priorityClassName: low
  prometheus:
    url: http://{{ .Release.Name }}-prometheus-prometheus
  resources:
    limits:
      cpu: 50m
      memory: 100Mi
    requests:
      cpu: 25m
      memory: 50Mi
  rules:
    default: false
    custom:  # This is a custom rule which exposes (requests_per_second) from the prometheus adapter to be used by any Horizontal Pod Autoscaler.
      - seriesQuery: 'api_all_request_total{namespace!="",pod!=""}'
        resources:
          overrides:
            namespace:
              resource: "namespace"
            pod:
              resource: "pod"
        name:
          as: "requests_per_second"
        metricsQuery: "sum(rate(<<.Series>>{<<.LabelMatchers>>}[3m])) by (<<.GroupBy>>)"

prometheus:
  alertmanager:
    alertmanagerSpec:
      alertmanagerConfigSelector:
        matchLabels:
          app.kubernetes.io/part-of: hedera-mirror-node
      priorityClassName: low
      resources:
        limits:
          cpu: 50m
          memory: 80Mi
        requests:
          cpu: 30m
          memory: 30Mi
    config:
      receivers:
        - name: 'null'
      route:
        group_by:
          - namespace
          - alertname
        group_wait: 30s
        receiver: 'null'
        repeat_interval: 7d
        routes: []
      templates:
        - '/etc/alertmanager/config/slack.tmpl'
    enabled: true
    templateFiles:
      slack.tmpl: |-
        {{- define "slack.title" -}}
        {{- .Status | title }} {{ .CommonLabels.alertname }}{{ if .CommonLabels.namespace }} in {{ with .CommonLabels.cluster }}{{ . }}/{{ end }}{{ .CommonLabels.namespace }}{{ end }}
        {{- end -}}

        {{- define "slack.text" -}}
        {{ range .Alerts -}}
        *Summary:* {{ with .Annotations.summary }}{{ . }}{{ else }}{{ .Annotations.message }}{{ end }} <{{ .GeneratorURL }}|:fire:> {{- with .Annotations.dashboard_url }}<{{ . }}|:chart_with_upwards_trend:>{{ end }} {{- with .Annotations.runbook_url }}<{{ . }}|:notebook:>{{ end }}{{"\n"}}
        {{- with .Annotations.description -}} *Description:* {{ . }}{{"\n"}}{{ end }}
        {{ end }}
        {{- end -}}
  coreDns:
    enabled: false
  enabled: true
  grafana:
    additionalDataSources:
      - name: AlertManager
        type: camptocamp-prometheus-alertmanager-datasource
        access: proxy
        url: http://{{ .Release.Name }}-prometheus-alertmanager:9093
      - name: Loki
        type: loki
        access: proxy
        url: http://{{ .Release.Name }}-loki:3100
        jsonData:
          maxLines: 500
    adminPassword: ""  # Randomly generated if left blank
    defaultDashboardsEnabled: true
    plugins:
      - camptocamp-prometheus-alertmanager-datasource
    resources:
      limits:
        cpu: 300m
        memory: 500Mi
      requests:
        cpu: 150m
        memory: 75Mi
  kube-state-metrics:
    resources:
      limits:
        cpu: 20m
        memory: 128Mi
      requests:
        cpu: 10m
        memory: 64Mi
  # We disable these exporters because they either don't work in GKE or produce too many time series
  kubeApiServer:
    enabled: false
  kubeControllerManager:
    enabled: false
  kubeDns:
    enabled: false
  kubeEtcd:
    enabled: false
  kubelet:
    # Disable these high cardinality metrics
    serviceMonitor:
      cAdvisorMetricRelabelings:
        - action: drop
          regex: container_(memory_failures_total|tasks_state)
          sourceLabels: [__name__]
      metricRelabelings:
        - action: drop
          regex: .*_bucket
          sourceLabels: [__name__]
  kubeProxy:
    enabled: false
  kubeScheduler:
    enabled: false
  prometheus-node-exporter:
    hostNetwork: false
    priorityClassName: critical
    prometheus:
      monitor:
        relabelings:
          - sourceLabels: [__meta_kubernetes_endpoint_node_name]
            targetLabel: node
    resources:
      limits:
        cpu: 100m
        memory: 50Mi
      requests:
        cpu: 50m
        memory: 20Mi
  prometheus:
    additionalPodMonitors:
      - name: traefik
        podMetricsEndpoints:
          - port: metrics
            path: /metrics
            interval: 15s
        selector:
          matchLabels:
            app.kubernetes.io/name: traefik
      - name: stackgres-collector
        podMetricsEndpoints:
          - honorLabels: true
            honorTimestamps: true
            interval: 1m
            metricRelabelings:
              - action: drop
                regex: ^(pg_stat.*_user.*|pg_table.*|pgbouncer_show_clients.*)$
                sourceLabels:
                  - __name__
            scrapeTimeout: 30s
            path: /metrics
            port: prom-http
            scheme: https
            tlsConfig:
              ca:
                secret:
                  key: tls.crt
                  name: stackgres-collector
              serverName: stackgres-collector
        selector:
          matchLabels:
            app: StackGresConfig
            stackgres.io/collector: "true"
    prometheusSpec:
      podMonitorSelectorNilUsesHelmValues: false
      priorityClassName: high
      resources:
        limits:
          cpu: 750m
          memory: 4Gi
        requests:
          cpu: 250m
          memory: 1Gi
      retentionSize: 84GiB  # Capacity minus 16% so KubePersistentVolumeFillingUp alert doesn't fire
      ruleSelectorNilUsesHelmValues: false
      scrapeInterval: 30s
      serviceMonitorSelectorNilUsesHelmValues: false
      storageSpec:
        volumeClaimTemplate:
          spec:
            resources:
              requests:
                storage: 100Gi
      walCompression: true
  prometheusOperator:
    resources:
      limits:
        cpu: 100m
        memory: 200Mi
      requests:
        cpu: 50m
        memory: 50Mi

promtail:
  config:
    clients:
      - url: http://{{ .Release.Name }}-loki:3100/loki/api/v1/push
    snippets:
      pipelineStages:
        - cri: {}
        - multiline:
            firstline: '^\d{4}-\d{2}-\d{2}T\d{1,2}:\d{2}:\d{2}\.\d+(Z|[+-]\d{4}) '
  enabled: true
  priorityClassName: critical
  resources:
    limits:
      cpu: 125m
      memory: 150Mi
    requests:
      cpu: 50m
      memory: 50Mi
  serviceMonitor:
    enabled: true
  tolerations:
    - effect: NoSchedule
      operator: Exists

stackgres:
  cert:
    collectorSecretName: stackgres-collector
  collector:
    config:
      exporters:
        prometheus:
          metric_expiration: 2m
      service:
        pipelines:
          metrics:
            receivers:
              - prometheus
    name: stackgres-collector
    prometheusOperator:
      allowDiscovery: false
      monitors: []  # Configured as part of the prometheus chart to enable finer control over scrape endpoints
    resources:
      limits:
        cpu: "1500m"
        memory: "4Gi"
      requests:
        cpu: "1000m"
        memory: "3Gi"
  enabled: false
  operator:
    image:
      name: gcr.io/mirrornode/stackgres-operator
  prometheusRules:
    enabled: true
    DatabaseStorageFull:
      annotations:
        description: Storage for {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is {{ $value | humanizePercentage }} full
        summary: Database storage is full
      enabled: true
      expr: kubelet_volume_stats_used_bytes{node=~".*(worker|coord).*"} / kubelet_volume_stats_capacity_bytes{node=~".*(worker|coord).*"} >= .80
      for: 1m
      labels:
        severity: critical
        application: hedera-mirror-common
        area: resource

testkube:
  enabled: false
  executor:
    image:
      registry: docker.io
      repository: grafana/k6
      tag: 0.53.0
  namespace: testkube
  test:
    config:
      rest: {}
      restjava: {}
      web3: {}
    delay: 600s  # Delay duration between test suite steps. This needs to be at least 600s to match the pgbouncer server
                 # connection idle timeout, so that connections will be freed up for the next set of tests
    extraExecutionRequestVariables: {}
    gitBranch: ""  # Default to .Chart.AppVersion
    schedule: ""  # Cron schedule of the test suite, default to no schedule
    targets: []
    #  - namespace: ""
    #    release: mirror
  trigger:
    enabled: false

traefik:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: traefik
  deployment:
    kind: DaemonSet
  enabled: true
  globalArguments:  # Expose X-Forwarded-For header for tracing
    - --entryPoints.web.forwardedHeaders.insecure
    - --entryPoints.websecure.forwardedHeaders.insecure
    - --api.insecure=true
  logs:
    access:
      enabled: true
      filters:
        statuscodes: 400-599
  metrics:
    prometheus:
      addServicesLabels: true
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
  ports:
    websecure:
      tls:
        enabled: true
  priorityClassName: critical
  resources:
    limits:
      cpu: 1500m
      memory: 2000Mi
    requests:
      cpu: 1000m
      memory: 500Mi
  service:
    spec: {}
    type: ClusterIP

zfs:
  coordinator:
    initialDiskSize: 300GB
  enabled: false
  engines:
    local:
      lvm:
        enabled: false
      zfs:
        enabled: true
    replicated:
      mayastor:
        enabled: false
  # ZFS daemonset bootstrap configuration
  init:
    arcSizeGb: 2
    diskPrefix: citus
    image:
      pullPolicy: IfNotPresent
      registry: gcr.io
      repository: google.com/cloudsdktool/google-cloud-cli
      tag: slim
    l2ArcNvmeDeviceId: ""
    serviceAccount:
      name: zfs-service-account
      iAmName: sa-name@project.iam.gserviceaccount.com
  lvm-localpv:
    enabled: false
  mayastor:
    enabled: false
  openebs-crds:
    csi:
      volumeSnapshots:
        enabled: false
        keep: true  # Keep zfs resources when chart is uninstalled
  parameters:
    compression: zstd-6
    fstype: zfs
    poolname: zfspv-pool
    recordsize: 32k
  priorityClassName: critical
  worker:
    initialDiskSize: 350GB
  zfs-localpv:
    analytics:
      enabled: false
    zfsController:
      priorityClass:
        create: true
    zfsNode:
      additionalVolumes:
        node:
          hostPath:
            path: /
        scripts:
          configMap:
            name: "{{ .Release.Name }}-zfs-init"
            defaultMode: 0744
      initContainers:
        label-wait:
          image: gcr.io/google.com/cloudsdktool/google-cloud-cli:slim
          imagePullPolicy: IfNotPresent
          command: ["/scripts/label-wait.sh"]
          env:
            - name: ROOT_MOUNT_DIR
              value: /node
          securityContext:
            privileged: true
          volumeMounts:
            - name: node
              mountPath: /node
            - name: scripts
              mountPath: /scripts
      nodeSelector:
        csi-type: zfs
      podLabels:
        component: openebs-zfs-node
      priorityClass:
        create: true
      tolerations:
        - effect: NoSchedule
          key: zfs
          operator: Equal
          value: "true"
