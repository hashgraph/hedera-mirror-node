# Overview

Hedera Mirror Node receives and stores transactions generated by the [Hedera](https://hedera.com) network and
exposes them via gRPC and REST APIs. While mirror nodes receive information from the main nodes, they do not contribute
to consensus on the mainnet and their votes are not counted. Only the votes from the main nodes are counted for
determining consensus. The trust of Hedera mainnet is derived based on the the consensus reached by the mainnet nodes.
That trust is transferred to the mirror nodes using cryptographic signatures on a chain of records (account balances,
events, transactions, etc).

For more information, visit our [GitHub](https://github.com/hashgraph/hedera-mirror-node) project.

For one click deployment to Google Kubernetes Engine see our
[Google Cloud Platform Marketplace solution](https://console.cloud.google.com/marketplace/details/mirror-node-public/hedera-mirror-node).

For information on developing and testing the Marketplace solution, see [BUILDING.md](BUILDING.md).

## Architecture

![Architecture diagram](architecture.png)

The Hedera Mirror Node is composed of the following components:

### Importer

Receives data from the main nodes and persists it to the database. A Kubernetes
[StatefulSet](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) is used to manage the Importer
component. It does not currently support scaling to more than one replica. A
[PersistentVolume](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is associated with each Pod and
stores the current address book along with the downloaded balance, event and record files.

### gRPC API

Serves the Hedera Consensus Service (HCS) streaming API. A Kubernetes
[Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) is used to manage the gRPC component.
This API is exposed via a LoadBalancer Service on port 5600. It can be horizontally scaled by adjusting its replica count.
See the gRPC API [documentation](https://docs.hedera.com/guides/docs/mirror-node-api/hedera-consensus-service-api-1)
for more information on how to invoke the API.

### REST API

Serves account, balance, topic message and transaction APIs. A Kubernetes
[Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) is used to manage the gRPC component.
This API is exposed via a LoadBalancer Service on port 80. It can be horizontally scaled by adjusting its replica count.
See the REST API [documentation](https://docs.hedera.com/guides/docs/mirror-node-api/cryptocurrency-api) for more
information on how to invoke the API.

### PostgreSQL

SQL database used to store the historical transaction data. The database is not exposed externally but you can connect
internally via port 5432. The current configuration does not currently support more than one replica. A
[PersistentVolume](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is associated with each Pod.

# Setup

## Command-line tools

You'll need the following tools in your development environment:

- [docker](https://docs.docker.com/install/)
- [git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
- [gcloud](https://cloud.google.com/sdk/gcloud/)
- [grpcurl](https://github.com/fullstorydev/grpcurl)
- [Helm 3](https://helm.sh/)
- [kubectl](https://kubernetes.io/docs/reference/kubectl/overview/)

> **NOTE:** We recommend using a `kubectl` version that is the same as the version of your cluster. Using the same
> version for `kubectl` and the cluster helps to avoid unforeseen issues.

## Set environment variables

Setup common environment variables that will be used for the remainder of the document:

```shell
export APP_NAME="mirror-1"
export CLUSTER="hedera-cluster"
export NAMESPACE="test"
export ZONE="us-central1-b"
```

## Create a Google Kubernetes Engine (GKE) cluster

Use `gcloud` to login to your Google account:

```shell
gcloud auth login
```

Create a new cluster from the command-line:

```shell
gcloud container clusters create "${CLUSTER}" --zone "${ZONE}"
```

Configure `kubectl` to connect to the new cluster:

```shell
gcloud container clusters get-credentials "${CLUSTER}" --zone "${ZONE}"
```

## Create Kubernetes namespace

If you use a different namespace than the `default`, run the command below to create a new namespace:

```shell
kubectl create namespace "${NAMESPACE}"
```

## Obtain the Helm chart

Clone this repository and checkout the latest stable tag:

```shell
git clone https://github.com/hashgraph/hedera-mirror-node.git
git checkout tags/v0.12.0 # Change version appropriately
```

## Install the Application resource definition

An Application resource is a collection of individual Kubernetes components, such as
Services, StatefulSets, and so on, that you can manage as a group.

To set up your cluster to understand Application resources, run the following command:

```shell
kubectl apply -f "https://raw.githubusercontent.com/GoogleCloudPlatform/marketplace-k8s-app-tools/master/crd/app-crd.yaml"
```

You only need to run this command once.

The Application resource is defined by the
[Kubernetes SIG-apps](https://github.com/kubernetes/community/tree/master/sig-apps)
community. You can find the source code at
[github.com/kubernetes-sigs/application](https://github.com/kubernetes-sigs/application).

# Installation

## Quick install with Google Cloud Marketplace

Get up and running with a few clicks! To install this app to a Google Kubernetes Engine cluster via Google Cloud
Marketplace, follow these [on-screen instructions](https://console.cloud.google.com/marketplace/details/mirror-node-public/hedera-mirror-node).

## Command-line instructions

You can use [Google Cloud Shell](https://cloud.google.com/shell/) or a local workstation to follow the steps below.

[![Open in Cloud Shell](http://gstatic.com/cloudssh/images/open-btn.svg)](https://console.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https://github.com/hashgraph/hedera-mirror-node&cloudshell_open_in_editor=README.md&cloudshell_working_dir=charts/marketplace/gcp)

### Configure

Add the Hedera Mirror Node Helm chart repository:

```shell
helm repo add hedera https://hashgraph.github.io/hedera-mirror-node/charts
```

Optional: Use a function to generate the password for the components:

```shell
alias generate_password="cat /dev/urandom | LC_CTYPE=C tr -dc a-zA-Z0-9 | head -c 24"
```

Create a custom values.yaml that can be reused for both installs and upgrades:

```shell
cat <<EOF > custom-values.yaml
global:
  rest:
    password: $(generate_password)
grpc:
  config:
    hedera:
      mirror:
        grpc:
          db:
            password: $(generate_password)
  rbac:
    enabled: true
  serviceAccount:
    create: true
importer:
  config:
    hedera:
      mirror:
        importer:
          db:
            password: $(generate_password)
  rbac:
    enabled: true
  serviceAccount:
    create: true
postgresql:
  postgresql:
    password: $(generate_password)
    repmgrPassword: $(generate_password)
rest:
  serviceAccount:
    create: true
EOF
```

Edit the `custom-values.yaml` with any other value you want to change. See the wrapper chart's [values.yaml](/charts/hedera-mirror/values.yaml)
for additional configuration options. You can pass any application configuration option listed in
[docs/configuration.md](/docs/configuration.md) via `grpc.config`, `importer.config` and `rest.config`.

### Perform the install

Use `helm upgrade` to install and upgrade the application:

```shell
helm upgrade "${APP_NAME}" hedera/hedera-mirror \
  --namespace "${NAMESPACE}" \
  --install \
  -f charts/marketplace/gcp/values.yaml \
  -f custom-values.yaml
```

Wait for all pods to become ready before continuing.

### Verify the install

You can use Helm tests to automatically verify the installation was successful:

```shell
helm test "${APP_NAME}" --namespace "${NAMESPACE}"
```

# Basic Usage

## View the app in the Google Cloud Console

To get the Cloud Console URL for your app, run the following command:

```shell
echo "https://console.cloud.google.com/kubernetes/application/${ZONE}/${CLUSTER}/${NAMESPACE}/${APP_NAME}"
```

To view the app, open the URL in your browser.

## Access the gRPC API

Get the external IP of your mirror Node gRPC API using the following command:

```shell
GRPC_IP="$(kubectl get "service/${APP_NAME}-grpc" -n "${NAMESPACE}" -o jsonpath='{.status.loadBalancer.ingress[0].ip}')"
grpcurl -plaintext "${GRPC_IP}:5600" list
```

## Access the REST API

Get the external IP of your Mirror Node REST API using the following command:

```shell
REST_IP="$(kubectl get "service/${APP_NAME}-rest" -n "${NAMESPACE}" -o jsonpath='{.status.loadBalancer.ingress[0].ip}')"
curl -s "http://${REST_IP}/api/v1/transactions?limit=1"
```

## Access PostgreSQL

```shell
kubectl exec -it -n "${NAMESPACE}" "${APP_NAME}-postgres-postgresql-0" -c postgresql -- psql -d mirror_node -U postgres
```

# Backup and Restore

## Backup Importer

```shell
SNAPSHOT_NAME=gke-${NAMESPACE}-${APP_NAME}-importer-0
VOLUME_NAME=$(kubectl get persistentvolumeclaim data-${APP_NAME}-importer-0 -n "${NAMESPACE}" -o jsonpath='{.spec.volumeName}')
DISK_NAME=$(kubectl get pv ${VOLUME_NAME} -n "${NAMESPACE}" -o jsonpath='{.spec.gcePersistentDisk.pdName}')
ZONE=$(kubectl get pv ${VOLUME_NAME} -n "${NAMESPACE}" -o jsonpath='{.metadata.labels.failure-domain\.beta\.kubernetes\.io/zone}')

kubectl scale -n "${NAMESPACE}" --replicas=0 "statefulset/${APP_NAME}-importer"
gcloud compute disks snapshot --snapshot-names "${SNAPSHOT_NAME}" --zone "${ZONE}" "${DISK_NAME}"
kubectl scale -n "${NAMESPACE}" --replicas=1 "statefulset/${APP_NAME}-importer"
```

## Backup PostgreSQL

To backup PostgreSQL, you can use a similar technique with gcloud snapshots or you can use pg_dumpall.
Replace the value of `PGPASSWORD` below with the password for the `postgres` superuser:

```shell script
kubectl exec -t -n "${NAMESPACE}" "${APP_NAME}-postgres-postgresql-0" -c postgresql -- \
  bash -c 'PGPASSWORD="password" pg_dumpall -c -l mirror_node -U postgres' > postgresql-backup.sql
```

> **Note:** You may want to scale the importer down before backing up the database so no writes are occurring

## Restore Importer

Set environment variables

```shell
SNAPSHOT_NAME=gke-${NAMESPACE}-${APP_NAME}-importer-0
VOLUME_NAME=$(kubectl get persistentvolumeclaim data-${APP_NAME}-importer-0 -n "${NAMESPACE}" -o jsonpath='{.spec.volumeName}')
DISK_NAME=$(kubectl get pv ${VOLUME_NAME} -n "${NAMESPACE}" -o jsonpath='{.spec.gcePersistentDisk.pdName}')
DISK_SIZE=$(gcloud compute disks list --filter="${DISK_NAME}" --format "value(size_gb)")
DISK_TYPE=$(gcloud compute disks list --filter="${DISK_NAME}" --format "value(type)")
```

Scale importer replicas to zero

```shell script
kubectl scale -n "${NAMESPACE}" --replicas=0 "statefulset/${APP_NAME}-importer"
```

Save the current manifests

```shell
kubectl get persistentvolumeclaim "data-${APP_NAME}-importer-0" -n "${NAMESPACE}" -o yaml | grep -vF "pv.kubernetes.io" > pvc.yaml
kubectl get persistentvolume "${VOLUME_NAME}" -n "${NAMESPACE}" -o yaml | grep -vE "(uid|resourceVersion)" > pv.yaml
```

Delete the persistent volume claim and associated volume

```shell
kubectl delete persistentvolumeclaim data-${APP_NAME}-importer-0 -n "${NAMESPACE}"
```

Create a new disk from snapshot

```shell script
gcloud compute disks create "${DISK_NAME}" \
  --size "${DISK_SIZE}" \
  --source-snapshot "${SNAPSHOT_NAME}" \
  --type "${DISK_TYPE}" \
  --zone "${ZONE}"
```

Restore the previous manifests

```shell
kubectl apply -f pv.yaml
kubectl apply -f pvc.yaml
```

Scale importer replicas back to one

```shell script
kubectl scale -n "${NAMESPACE}" --replicas=1 "statefulset/${APP_NAME}-importer"
```

## Restore PostgreSQL

First ensure the components that write to the database like the importer are scaled down:

```shell
kubectl scale -n "${NAMESPACE}" --replicas=0 "statefulset/${APP_NAME}-importer"
```

Then use `psql` inside the container to restore. Replace `PGPASSWORD` below with the password for the `postgres` superuser:

```shell
cat postgresql-backup.sql | \
  kubectl exec -it -n "${NAMESPACE}" "${APP_NAME}-postgres-postgresql-0" -c postgresql -- \
    bash -c 'PGPASSWORD="password" psql -U postgres'
```

Scale the importer back up

```shell
kubectl scale -n "${NAMESPACE}" --replicas=1 "statefulset/${APP_NAME}-importer"
```

# Upgrade

Obtain the version of the Helm chart you want to install by first changing to the directory
that contains the mirror node git repository then:

```shell
git checkout tags/v0.12.0 # Change version appropriately
```

The command to upgrade is the same to install. Make any necessary changes to the previously constructed
`custom-values.yaml` first, then run:

```shell
helm upgrade "${APP_NAME}" charts/hedera-mirror \
  --namespace "${NAMESPACE}" \
  --install \
  -f charts/marketplace/gcp/values.yaml \
  -f custom-values.yaml
```

# Scaling

Scaling the Importer or PostgreSQL components is not currently supported. The gRPC and REST
APIs support horizontal scaling by setting their corresponding replica count property. To adjust their replicas:

```shell
helm upgrade "${APP_NAME}" charts/hedera-mirror \
  --namespace "${NAMESPACE}" \
  --install \
  --reuse-values \
  --set grpc.replicas=3 \
  --set rest.replicas=10
```

# Uninstall

## Using the Google Cloud Console

1. In the Cloud Console, open [Kubernetes Applications](https://console.cloud.google.com/kubernetes/application).

2. From the list of apps, choose your app installation.

3. On the Application Details page, click **Delete**.

## Using the command-line

### Delete the application

Delete the Kubernetes resources using Helm:

```shell
helm delete "${APP_NAME}" --namespace "${NAMESPACE}"
```

### Delete the data

By design, removing StatefulSets in Kubernetes does not remove any PersistentVolumeClaims that were attached to their
Pods. This prevents your installations from accidentally deleting stateful data.

To remove the PersistentVolumeClaims with their attached persistent disks, run the following command:

```shell
kubectl delete persistentvolumeclaims --namespace "${NAMESPACE}" --selector "app.kubernetes.io/instance=${APP_NAME}"
```
