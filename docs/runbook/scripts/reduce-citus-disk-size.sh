#!/usr/bin/env bash
set -euox pipefail

source ./utils.sh

ONE_GI_BYTES=1073741824
EPOCH_SECONDS=$(date +%s)
ZFS_VOLUMES=$(getZFSVolumes)
NODE_ID_MAP=$(echo -e "${ZFS_VOLUMES}" |
              jq -r 'group_by(.nodeId) |
                       map((.[0].nodeId) as $nodeId |
                         map(.)|
                         {
                           ($nodeId) :  {
                              pvcs: (.)
                           }
                         }
                      )|add')

function configureAndValidate() {
  GCP_PROJECT="$(readUserInput "Enter GCP Project for target: ")"
  if [[ -z "${GCP_PROJECT}" ]]; then
    log "GCP_PROJECT is not set and is required. Exiting"
    exit 1
  else
    gcloud projects describe "${GCP_PROJECT}" >/dev/null
  fi

  GCP_K8S_CLUSTER_REGION="$(readUserInput "Enter target cluster region: ")"
  if [[ -z "${GCP_K8S_CLUSTER_REGION}" ]]; then
    log "GCP_K8S_CLUSTER_REGION is not set and is required. Exiting"
    exit 1
  else
    gcloud compute regions describe "${GCP_K8S_CLUSTER_REGION}" --project "${GCP_PROJECT}" >/dev/null
  fi

  GCP_K8S_CLUSTER_NAME="$(readUserInput "Enter target cluster name: ")"
  if [[ -z "${GCP_K8S_CLUSTER_NAME}" ]]; then
    log "GCP_K8S_CLUSTER_NAME is not set and is required. Exiting"
    exit 1
  else
    gcloud container clusters describe --project "${GCP_PROJECT}" \
      --region="${GCP_K8S_CLUSTER_REGION}" \
      "${GCP_K8S_CLUSTER_NAME}" >/dev/null
  fi

  DISK_PREFIX="$(kubectl get daemonsets \
  -n "${COMMON_NAMESPACE}" \
  -l 'app=zfs-init' \
  -o json |
  jq -r '.items[0].spec.template.spec.initContainers[0].env[] | select (.name == "DISK_PREFIX") | .value')"

  if [[ -z "${DISK_PREFIX}" ]]; then
    log "DISK_PREFIX can not be empty. Exiting"
    exit 1
  fi

  POOL_NAME="$(kubectl get storageclasses.storage.k8s.io zfs -o jsonpath='{.parameters.poolname}')"
  if [[ -z "${POOL_NAME}" ]]; then
    log "POOL_NAME can not be empty. Exiting"
    exit 1
  fi

  RECORD_SIZE="$(kubectl get storageclasses.storage.k8s.io zfs -o jsonpath='{.parameters.recordsize}')"
  if [[ -z "${RECORD_SIZE}" ]]; then
    log "RECORD_SIZE can not be empty. Exiting"
    exit 1
  fi

  COMPRESSION="$(kubectl get storageclasses.storage.k8s.io zfs -o jsonpath='{.parameters.compression}')"
  if [[ -z "${COMPRESSION}" ]]; then
    log "COMPRESSION can not be empty. Exiting"
    exit 1
  fi

  AVAILABLE_DISKS="$(gcloud compute disks list \
  --project "${GCP_PROJECT}" \
  --filter="name~${DISK_PREFIX}.*-zfs" \
  --format="json(name, sizeGb, users, zone)")"
  if [[ "${AVAILABLE_DISKS}" == "[]" ]]; then
    log "No disks found for prefix ${DISK_PREFIX}. Exiting"
    exit 1
  fi

  DISKS_TO_REDUCE_INPUT="$(readUserInput "Enter the disks(${AVAILABLE_DISKS}) to reduce (space-separated): ")"
  if [[ -z "${DISKS_TO_REDUCE_INPUT}" ]]; then
    log "POOLS_TO_UPDATE_INPUT is not set and is required. Exiting"
    exit 1
  else
    NEW_DISK_SIZES=()
    IFS=', ' read -r -a DISKS_TO_REDUCE <<<"${DISKS_TO_REDUCE_INPUT}"
    for disk in "${DISKS_TO_REDUCE[@]}"; do
      DISK_NODE_ID=${disk#"$DISK_PREFIX"-}
      DISK_NODE_ID=${DISK_NODE_ID%"-zfs"}
      DISK_ZONE=$(echo "${DISK_NODE_ID}" | cut -d '-' -f 2-4)
      gcloud compute disks describe "${disk}" \
      --project="${GCP_PROJECT}" \
      --zone="${DISK_ZONE}" >/dev/null

      DISK_SIZE="$(readUserInput "Enter new disk size for ${disk} in GB: ")"
      if [[ "$DISK_SIZE" =~ ^[0-9]*$ ]]; then
        NEW_DISK_SIZES+=("${DISK_SIZE}")
      else
        log "DISK_SIZE is not an integer is required. Exiting"
        exit 1
      fi
    done
  fi
}

function reduceDiskSizes() {
  for diskIndex in "${!DISKS_TO_REDUCE[@]}"; do
    DISK_NAME=${DISKS_TO_REDUCE[$diskIndex]}
    DISK_SIZE=${NEW_DISK_SIZES[$diskIndex]}

    DISK_NODE_ID=${DISK_NAME#"$DISK_PREFIX"-}
    DISK_NODE_ID=${DISK_NODE_ID%"-zfs"}
    DISK_ZONE=$(echo "${DISK_NODE_ID}" | cut -d '-' -f 2-4)

    INSTANCE_NAME="$(kubectl get zfsnodes.zfs.openebs.io \
    -n "${COMMON_NAMESPACE}" \
    -o jsonpath='{.metadata.ownerReferences[0].name}' \
    "${DISK_NODE_ID}" )"
    NEW_DISK_NAME="${DISK_NAME}-resize-${EPOCH_SECONDS}"
    gcloud compute disks create "${NEW_DISK_NAME}" \
                    --project="${GCP_PROJECT}" \
                    --type=pd-balanced \
                    --size="${DISK_SIZE}GB" \
                    --zone="${DISK_ZONE}"
    gcloud compute instances attach-disk "${INSTANCE_NAME}" \
    --disk "${NEW_DISK_NAME}" \
    --zone "${DISK_ZONE}" \
    --project "${GCP_PROJECT}" \
    --device-name=sdc
    cat >"/tmp/${INSTANCE_NAME}.yaml" <<EOF
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ${INSTANCE_NAME}
  namespace: common
spec:
  selector:
    matchLabels:
      instance: ${INSTANCE_NAME}
  template:
    metadata:
      labels:
        instance: ${INSTANCE_NAME}
    spec:
      hostNetwork: true
      hostPID: true
      hostIPC: true
      containers:
        - name: busybox
          image: ubuntu
          securityContext:
            privileged: true
          command:
            - tail
            - "-f"
            - /dev/null
          volumeMounts:
            - name: nodefs
              mountPath: /node-fs
            - name: devfs
              mountPath: /dev
      volumes:
        - name: nodefs
          hostPath:
            path: /
        - name: devfs
          hostPath:
            path: /dev
      nodeSelector:
        kubernetes.io/hostname: ${INSTANCE_NAME}
      tolerations:
        - effect: NoSchedule
          key: zfs
          operator: Equal
          value: "true"
EOF

    kubectl apply -f "/tmp/${INSTANCE_NAME}.yaml"

    log "Waiting for helper pod to be ready"
    kubectl wait --for=condition=Ready pod -n "${COMMON_NAMESPACE}" -l "instance=${INSTANCE_NAME}" --timeout=-1s
    DAEMONSET_POD=$(kubectl get pods \
    -n "${COMMON_NAMESPACE}" \
    -l instance="${INSTANCE_NAME}" \
     -o jsonpath='{.items[*].metadata.name}')
    kubectl exec -n "${COMMON_NAMESPACE}" "${DAEMONSET_POD}" -- bash \
    -c "chroot /node-fs /bin/bash -c \
       'zpool create -o autoexpand=on ${POOL_NAME}-new /dev/sdc'"
    PVCS=$(echo "${NODE_ID_MAP}" | jq -r --arg DISK_NODE_ID "${DISK_NODE_ID}" '.[$DISK_NODE_ID].pvcs')
    NODE_PVC_COUNT=$(echo "${PVCS}" | jq -r 'length - 1')

    for pvcIndex in $(seq 0 "${NODE_PVC_COUNT}"); do
      PVC=$(echo "${PVCS}" | jq -r --arg INDEX "$pvcIndex" '.[$INDEX | tonumber]')
      PVC_VOLUME=$(echo "${PVC}" | jq -r '.volumeName')
      CURRENT_PVC_SIZE="$(echo "${PVC}" | jq -r '.pvcSize')"
      CURRENT_PVC_SIZE="${CURRENT_PVC_SIZE%Gi}"
      NEW_PVC_SIZE="$(readUserInput "Enter new size for ${PVC_VOLUME} in Gi(current size: ${CURRENT_PVC_SIZE}): ")"
      while ! [[ "${NEW_PVC_SIZE}" =~ ^[0-9]*$ || "${NEW_PVC_SIZE}" -lt "${CURRENT_PVC_SIZE}" ]]; do
        log "NEW_PVC_SIZE is not an integer or is greater than the current size"
        NEW_PVC_SIZE="$(readUserInput "Enter new size for ${PVC_VOLUME} in Gi(current size: ${CURRENT_PVC_SIZE}): ")"
      done
      RESERVATION=$((ONE_GI_BYTES * NEW_PVC_SIZE))
      if [[ "${NEW_PVC_SIZE}" -ne "${CURRENT_PVC_SIZE}" ]]; then
        NEW_VOLUMES+=("$(echo "${PVC}" |
          jq -r --arg RESERVATION "${RESERVATION}" \
            --arg NEW_PVC_SIZE "${NEW_PVC_SIZE}Gi" \
            '. += {newPvcSize: $NEW_PVC_SIZE} |
             . += {newPvcSizeBytes: $RESERVATION}')")
      fi
      kubectl exec -it -n "${COMMON_NAMESPACE}" "${DAEMONSET_POD}" -- bash -c \
      "chroot /node-fs /bin/bash -c  \
      'zfs snapshot ${POOL_NAME}/${PVC_VOLUME}@initial && \
      zfs send ${POOL_NAME}/${PVC_VOLUME}@initial | pv | \
      zfs receive \
      -o compression=${COMPRESSION} \
      -o recordsize=${RECORD_SIZE} \
      -o reservation=${RESERVATION} \
      -o mountpoint=legacy \
      ${POOL_NAME}-new/${PVC_VOLUME} && \
      zfs destroy ${POOL_NAME}-new/${PVC_VOLUME}@initial'"

      log "Finished copy for pvc ${PVC_VOLUME} to ${POOL_NAME}-new/${PVC_VOLUME}"
    done

    kubectl exec -it -n "${COMMON_NAMESPACE}" "${DAEMONSET_POD}" -- bash -c \
      "chroot /node-fs /bin/bash -c \
      'zpool destroy ${POOL_NAME} && \
       zpool export ${POOL_NAME}-new && \
       zpool import ${POOL_NAME}-new ${POOL_NAME} && \
       zpool export ${POOL_NAME}'"

    DISK_REGION=$(echo "${DISK_ZONE}" | cut -d '-' -f 1-2)
    gcloud compute snapshots create "${NEW_DISK_NAME}" --project="${GCP_PROJECT}" --source-disk="${NEW_DISK_NAME}" \
    --source-disk-zone="${DISK_ZONE}" --storage-location="${DISK_REGION}" --description="Resize for ${DISK_NAME}" \
    --quiet
    gcloud compute instances detach-disk "${INSTANCE_NAME}" --disk="${DISK_NAME}" --zone="${DISK_ZONE}" \
    --project="${GCP_PROJECT}" --quiet
    gcloud compute disks delete "${DISK_NAME}" \
    --zone="${DISK_ZONE}" \
    --project="${GCP_PROJECT}" \
    --quiet
    gcloud compute disks create "${DISK_NAME}" \
    --project="${GCP_PROJECT}" \
    --zone="${DISK_ZONE}" \
    --source-snapshot=projects/"${GCP_PROJECT}"/global/snapshots/"${NEW_DISK_NAME}" --type=pd-balanced
    gcloud compute instances detach-disk "${INSTANCE_NAME}" \
    --disk="${NEW_DISK_NAME}" --zone="${DISK_ZONE}" \
    --project="${GCP_PROJECT}" --quiet
    gcloud compute disks delete "${NEW_DISK_NAME}" \
    --zone="${DISK_ZONE}" \
    --project="${GCP_PROJECT}" \
    --quiet
    gcloud compute instances attach-disk "${INSTANCE_NAME}" \
    --disk="${DISK_NAME}" \
    --zone="${DISK_ZONE}" \
    --project="${GCP_PROJECT}" \
    --quiet \
    --device-name=sdb

    kubectl exec -it -n "${COMMON_NAMESPACE}" "${DAEMONSET_POD}" -- bash -c \
    "chroot /node-fs /bin/bash -c \
    'zpool import ${POOL_NAME} && \
     zpool upgrade ${POOL_NAME}'"
    kubectl delete -n "${COMMON_NAMESPACE}" daemonset "${INSTANCE_NAME}"
  done
}

function updateK8sResources() {
  kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io \
  "${HELM_RELEASE_NAME}" -o yaml > "/tmp/sg-webhook-${EPOCH_SECONDS}.yaml"
  kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io "${HELM_RELEASE_NAME}"

  for newVolume in "${NEW_VOLUMES[@]}"; do
    PVC_NAME=$(echo "${newVolume}" | jq -r '.pvcName')
    PV_NAME=$(echo "${newVolume}" | jq -r '.volumeName')
    NAMESPACE=$(echo "${newVolume}" | jq -r '.namespace')
    NEW_PVC_SIZE=$(echo "${newVolume}" | jq -r '.newPvcSize')
    NEW_PVC_SIZE_BYTES=$(echo "${newVolume}" | jq -r '.newPvcSizeBytes')
    NEW_PVC=$(kubectl get pvc "${PVC_NAME}" -n "${NAMESPACE}" -o json |
    jq --arg NEW_PVC_SIZE "${NEW_PVC_SIZE}" '.spec.resources.requests.storage = $NEW_PVC_SIZE|
                                             del(.status)|
                                             del(.metadata.annotations["pv.kubernetes.io/bind-completed"])')
    kubectl get pvc -n "${NAMESPACE}" "${PVC_NAME}" -o yaml >"/tmp/${PVC_NAME}.yaml.bak"
    kubectl delete pvc -n "${NAMESPACE}" "${PVC_NAME}"

    kubectl patch pv "${PV_NAME}" --type='json' -p \
    '[{ "op": "replace", "path": "/spec/capacity/storage", "value": "'"${NEW_PVC_SIZE}"'" },
      { "op": "remove", "path": "/spec/claimRef" },
      { "op": "remove", "path": "/status" }]'

    kubectl patch zfsvolumes.zfs.openebs.io -n "${COMMON_NAMESPACE}" "${PV_NAME}"\
    --type='json' -p '[{ "op": "replace", "path": "/spec/capacity", "value": "'"${NEW_PVC_SIZE_BYTES}"'" }]'

    echo "${NEW_PVC}" | kubectl apply -f -

    SHARDED_CLUSTER="$(echo "${newVolume}" | jq -r '.citusCluster.shardedClusterName')"
    CLUSTER="$(echo "${newVolume}" | jq -r '.citusCluster.clusterName')"

    kubectl patch sgclusters -n "${NAMESPACE}" "${CLUSTER}" \
    --type='json' -p '[{ "op": "replace", "path": "/spec/pods/persistentVolume/size", "value": "'"${NEW_PVC_SIZE}"'" }]'

    IS_WORKER="$(echo "${newVolume}" | jq -r '.citusCluster.isCoordinator | not')"
    WORKER_OVERRIDES=$(kubectl get sgshardedclusters.stackgres.io -n "${NAMESPACE}" \
                      "${SHARDED_CLUSTER}" -o jsonpath='{.spec.shards.overrides}')
    COORDINATOR_SIZE="$(kubectl get sgshardedclusters.stackgres.io -n "${NAMESPACE}" \
                       "${SHARDED_CLUSTER}" -o jsonpath='{.spec.coordinator.pods.persistentVolume.size}')"
    if [[ "${IS_WORKER}" == "true" ]]; then
      WORKER_DEFAULT_SIZE="$(kubectl get sgshardedclusters.stackgres.io -n "${NAMESPACE}" \
                            "${SHARDED_CLUSTER}" -o jsonpath='{.spec.shards.pods.persistentVolume.size}')"
      if [[ -z "${WORKER_OVERRIDES}" ]]; then
        WORKER_OVERRIDES="[]"
      fi

      WORKER_OVERRIDES=$(echo "${WORKER_OVERRIDES}" | jq 'sort_by(.index)')
      if [[ "${NEW_PVC_SIZE%Gi}" -ne "${WORKER_DEFAULT_SIZE%Gi}" ]]; then
        INDEX=$(echo "${CLUSTER}" | grep -oE '[0-9]+$')
        PVC_OVERRIDE=$(echo "${WORKER_OVERRIDES}" | jq -r --arg INDEX "${INDEX}" \
        '.[] | select(.index == ($INDEX|tonumber))')

        if [[ -z "${PVC_OVERRIDE}" ]]; then
          PVC_OVERRIDE="{\"index\": ${INDEX}, \"pods\": {\"persistentVolume\": {\"size\": \"${NEW_PVC_SIZE}\"}}}"
          WORKER_OVERRIDES=$(echo "${WORKER_OVERRIDES}" | jq ". += [${PVC_OVERRIDE}]")
        else
          WORKER_OVERRIDES=$(echo "${WORKER_OVERRIDES}" |
          jq --arg NEW_PVC_SIZE "${NEW_PVC_SIZE}" --arg INDEX "${INDEX}" \
          '(.[] | select(.index == ($INDEX|tonumber)).pods.persistentVolume).size |= $NEW_PVC_SIZE')
        fi
      fi
    else
      COORDINATOR_SIZE="${NEW_PVC_SIZE}"
    fi

    SHARDED_CLUSTER_PATCH="{
                               \"spec\": {
                                 \"coordinator\": {
                                   \"pods\": {
                                     \"persistentVolume\": {
                                       \"size\": \"${COORDINATOR_SIZE}\"
                                     }
                                   }
                                 },
                                 \"shards\": {
                                   \"overrides\": ${WORKER_OVERRIDES}
                                 }
                              }
                          }"
      log "Patching sharded cluster ${SHARDED_CLUSTER} in namespace ${NAMESPACE} with ${SHARDED_CLUSTER_PATCH}"
      kubectl patch sgshardedclusters.stackgres.io "${SHARDED_CLUSTER}" \
       -n "${NAMESPACE}" \
       --type merge \
       -p "${SHARDED_CLUSTER_PATCH}"
      log "
      **** IMPORTANT ****
      Please configure your helm values.yaml for namespace ${namespace} to have the following values:

      stackgres.coordinator.pods.persistentVolume.size=${COORDINATOR_SIZE}

      stackgres.worker.overrides=${WORKER_OVERRIDES}
      "
      log "Continue to acknowledge config change is saved to be applied on next helm release"
      doContinue

  done
  kubectl apply -f "/tmp/sg-webhook-${EPOCH_SECONDS}.yaml"
}

configureAndValidate

NAMESPACES=($(echo $ZFS_VOLUMES | jq -r '.[].namespace' | tr ' ' '\n' | sort -u | tr '\n' ' '))
for namespace in "${NAMESPACES[@]}"; do
  unrouteTraffic "${namespace}"
  pauseCitus "${namespace}"
done

NEW_VOLUMES=()
reduceDiskSizes
updateK8sResources

log "Restarting stackgres pod"
kubectl delete pods -n "${COMMON_NAMESPACE}" -l 'app=StackGresConfig'
sleep 5
kubectl wait --for=condition=Ready pod -n "${COMMON_NAMESPACE}" -l 'app=StackGresConfig' --timeout=-1s

log "Restarting zfs-init daemonset"
kubectl delete pods -n "${COMMON_NAMESPACE}" -l 'app=zfs-init'
sleep 5
kubectl wait --for=condition=Ready pod -n "${COMMON_NAMESPACE}" -l 'app=zfs-init' --timeout=-1s

log "Restarting zfs node pods"
kubectl delete pods -n "${COMMON_NAMESPACE}" -l 'component=openebs-zfs-node'
sleep 5
kubectl wait --for=condition=Ready pod -n "${COMMON_NAMESPACE}" -l 'component=openebs-zfs-node' --timeout=-1s

NAMESPACES=($(echo $ZFS_VOLUMES | jq -r '.[].namespace' | tr ' ' '\n' | sort -u | tr '\n' ' '))
for namespace in "${NAMESPACES[@]}"; do
  unpauseCitus "${namespace}"
  routeTraffic "${namespace}"
done